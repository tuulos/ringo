
.. |date| date::

Ringo
'''''

.. contents::

Introduction
============

Ringo is a distributed, replicating key-value store based on consistent
hashing and immutable data. Unlike many general-purpose databases,
Ringo is designed for a very specific use case: For archiving small
(less than 4KB) or medium-size data items (<100MB) in real-time so that
the data can survive N - 1 disk breaks without any downtime, in a manner
that scales to terabytes of data. In addition to storing, Ringo should be
able to retrieve individual or small sets of data items with low latencies
(<10ms) and provide a convenient on-disk format for bulk data access.

Ringo supports the following operations on data: 

- **Create** a domain, which initializes a new set of items.
- **Put** item to a domain, which appends a new key-value pair to an 
  existing domain.
- **Get** items from a domain, which returns all values for the given key 
  from the given domain.

Note that the above operations can only add new items, or access existing
items in the system but never modify them. Furthermore, Ringo doesn't
change or move data internally in any way once it has been written to
disk. This should guarantee that Ringo never corrupts data. Even if the
local filesystem corrupts, which Ringo uses to store its data, the data
can be recovered from redundant copies on replica nodes.

Ringo is designed to work in a cluster of servers. It is based on
consistent hashing, which connects many independent processes on many
independent servers to a single consistent system. It is possible
to add and remove servers from the system on the fly without any
interruptions. This eases maintenance of the system and makes it
fault-tolerant, as there aren't any single points of failure. As the
system lacks a central bookkeeping mechanism, and it doesn't rely on any
global data structures, Ringo is inherently scalable. Simple chunking
mechanism takes care of load balancing and data distribution.

This document describes the internals of Ringo. The main purpose is
to help the reader understand the big picture before diving into the
source code.

Sections below contain references to corresponding modules and functions
in the source code in parentheses. For brevity, request handling
functions of form (``ringo_node:handle_cast(match..``) are written as
(``ringo_node:<match>``).

Ring
====

This section explains Ringo's ring-shaped overlay network which is based
on consistent hashing. In the following, this overlay network is simply
referred as "the ring". The related code can be found in the module
``ringo_node``.

The ring serves a single purpose: To provide a consistent addressing
scheme for data replication. Consider the following situation: You want
to make K redundant copies of a data item to N separate servers. If K =
N, you could easily broadcast data to all active servers. However, this
makes scaling up the storage space almost impossible, as K increases
linearly with the number of servers and redundant copies of old data
fill up any new servers that are added to the system.

In a more realistic case, K is a constant and K < N. How to choose
which K servers, or nodes, should receive a copy of the data item? You
could pre-allocate the key-space amongst the known nodes, but making
the allocation beforehand is difficult without knowing if the keys are
distributed uniformly, which keys will be used, and which nodes are
going to crash, when, and how many nodes will be added. Instead of a
static allocation, we would like to choose the K servers on the fly,
based on the current status of the system.

Consistent hashing provides a conceptually simple solution to the problem:

- Each node is given a random ID, in Ringo, a 128-bit integer,
  (``#rnode.myid``).
- Nodes are sorted according to ascending ID.
- Each node connects only to two other nodes: Its successor and predecessor
  in the list of active IDs. (``ringo_node:assign_node()``).
- The node with the largest ID connects back to the node with the smallest
  ID, closing the ring.
- If a node X dies, the ring is fixed by making X's predecessor its
  successor's predecessor, like in a linked list. A new node can be added to
  the ring in a similar manner.
- Due to the ordering, each node has an unambiguous position amongst its
  peers, which makes the system consistent.

This schema is illustrated below:

::
   
              +-----+
        +---->|  9  |-----+
        |     +-----+     V
     +-----+           +-----+ 
     |  6  |           |  2  |
     +-----+           +-----+
        ^     +-----+     |
        +-----|  3  |<----+
              +-----+

Each box is a node and the numbers inside the boxes are the node IDs. The
ring is used as follows:

- Each request is given a random ID (``ringo_util:domain_id()``).
- The request is sent to *any* node in the ring.
- A match function (``ringo_util:match()``) compares the request id Q and the
  node id P. A node handles requests where P < Q < P + 1 where P + 1 is the
  node's successor's ID (``ringo_node:<match>``).

For instance in the diagram above, a request with Q=5 would be handled
by the node 3 and a request Q=12 would be handled by the node 9, and Q=1
by the node 2. We call a node P that matches a request ID Q **owner**
for the request ID. In Ringo, each domain is unambiguously owned
by a single node by utilizing this mechanism. Note, however, that the
owner may change whenever a new node is added, or an existing node is
removed from the ring.

Building and maintaining the ring in practice is not quite as
straightforward. There are two main problems:

1. How to find the correct place in the ring for a new node that
doesn't know its neighbors?

2. How to ensure that only a single ring exists globally? For instance, in
the diagram above we could easily form two independent and internally
consistent rings (e.g. 9-2 and 3-6).

The first issue is solved by ``ringo_node:join_existing_ring()`` which is
called when the node starts up. This function obtains a list of available
nodes from the Erlang name server with ``ringo_util:ringo_nodes()``,
chooses the best matching neighbor candidates from the list, and
opportunistically tries to connect to them. If the neighbors accept the
new node, the node becomes a part of the ring. If not, the process is
repeated with new neighbor candidates. If the node runs out of candidates
before succeeding, it kills itself.

The second issue is handled by a separate process running under
``ringo_node``, called ``check_parallel_rings()``. Detecting parallel rings
is rather straightforward, but once a parallel ring has been detected, which
of the rings should be killed and which one should stay alive? As each
existing ring is running a ``check_parallel_rings()`` process of its own,
they are effectively racing to kill each other.

To prevent mutually assured destruction from happening, we need a globally
unique measure that unambiguously determines which ring is the only
correct one: We define that it is the ring that contains a node that has
the globally smallest ID. Since the node ID space contains only positive
integers, we know that a smallest ID always exists. Given this definition,
the job of ``check_parallel_ring()`` boils down to finding all parallel
rings, and killing all the rings that it finds, possibly also itself,
except the one that includes a node with the smallest ID.

Together, ``ringo_node():join_existing_ring()`` and
``ringo_node:check_parallel_rings()`` guarantee that an arbitrary number
of independent node processes, running on a number of separate servers,
eventually converge and form a single ring. For further details about
the ring formation, and healing during changes, see comments in the
``ringo_node`` module.


Node Structure
--------------

This section gives an overview of the modules in a ringo node. A ringo
node is a normal Linux process. A server may have several ringo nodes
running in parallel, which is often beneficial from load balancing
and resource utilization point of view, as described in the paper
about Amazon's Dynamo [Dynamo]_ that calls the processes *virtual
nodes*. However, data is guaranteed to be replicated on physically
separate servers regardless of the ring topology.

The diagram below depicts the core modules for a ringo node. The
hierarchy represents the supervision tree; each box corresponds to an
independent process or in some cases a few logically related utility
processes, all of which run in parallel.

::

                             +------------+
                             | ringo_main |
                             +------------+
                                    |
                             +------------+
                             | ringo_node |
                             +------------+
                                   ||| (many)
                            +--------------+
                            | ringo_domain |
                            +--------------+
             +----------------------+------------------+
             |                      |                  |
    +------------------+ +-------------------+ +----------------+
    | ringo_syncdomain | | ringo_indexdomain | | ringo_external |
    +------------------+ +-------------------+ +----------------+


When the Erlang virtual machines starts, it loads the ``ringo_main``
module which initializes the supervision tree. The main module starts a
single ``ringo_node`` process that represents the node in the ring. Each
node hosts multiple domains, namely the domains that match to the node's
ID range, and replicas for domains belonging to the node's successors.

Each domain is handled by a ``ringo_domain`` process. Domain
synchronization is handled by the ``ringo_syncdomain`` module, which
contains functions that are periodically executed in separate processes.
Detecting and copying of missing large, externally stored values is
handled by the ``ringo_external`` module. Get-requests are handled by
the indexing process in ``ringo_indexdomain``.


Creating a Domain
=================

A new domain is created with a POST-request:

``http://ringo/mon/data/domain_name?create``

POST-data is ignored and it may be empty.

After receiving the request, Ringo Gateway (``ringogw``) proceeds
as follows:

1. Domain ID is computed for the given ``domain_name``:

   ``domainid = md5("0 " + domain_name)``

   where 0 denotes the first chunk.

2. A ``new_domain`` request is sent to the ring for ``domainid``.

3. When a matching node is found, ``ringo_node`` spawns a new domain for this
   ``domainid``, unless it exists already, and calls
   ``ringo_domain:<new_domain>``. This function creates a new directory for
   the domain and initializes an empty DB file.

If the ``domainid`` exists already, the request fails.


Putting entries to a domain
===========================

A key-value pair is put to existing domain with a POST-request:

``http://ringo/mon/data/domain_name/key``

where POST-data contains the value. POST-data is taken as value as is, so
application is free to choose any encoding for the data.

In the simplest case, putting a key-value pair to an existing domain is
a straightforward operation: Domain ID is computed as above, a request
is sent to the domain owner, ``ringo_domain:<put>``, which appends the
entry to the DB file. In addition, the key and the entry's position in
the DB file is sent to ``ringo_indexdomain`` for indexing. If the domain
doesn't exists, the request fails.

Actual encoding of data is handled by ``ringo_writer``. Encoding adds
headers and checksums to the entry, but does not touch its contents. If
size of the value exceeds the limit for an *internal* entry, it is
written to an *external* file to the domain directory and a pointer to
it is saved to the DB. Currently values that are larger than 4KB are
saved outside the DB file.

In practice, processing of the put-requests is complicated by
*replication*, *chunking* and abrupt *changes in the ring*. These issues
are treated one by one below.

Replication
-----------

Each domain has an owner and K identical replicas, where K is defined
when the domain is created. Replicas are handled by the K nodes that
precede the owner in the ring, by ``ringo_domain`` process. As each
replica maintains an identical copy of the owner's data, any of them may
take over the owner in case that it fails. The resyncing process makes
sure that the owner and the replicas are kept in sync, even if the owner
or a replica may be temporarily unavailable.

Ringo relies on opportunistic replication. The domain owner
initiates replication in ``ringo_domain:<put>`` by calling
``ringo_domain:replicate()``. This function forwards the request to the
owner's predecessor in the ring using ``ringo_domain:<repl_put>``, which
in turn forwards the request to its predecessor etc. until K replicas
have been reached. The owner doesn't wait for acknowledgements from the
replicas, so there is no guarantee that any replicas have received or
written the entry successfully.

The rationale is the following: If none of the replicas, nor the owner,
can be found, the put request fails which gives the sender an opportunity
to retry the request later. If the owner is found, the request succeeds,
and it is likely that at least one copy of the entry has been stored
successfully. In this case, even if all the replicas would fail, resyncing
makes sure that additional copies will be created eventually. As long as
there are any nodes available in the ring besides the owner, replicas will be
created. Since Ringo is so greedy in making replicas anyway, notifying the
sender about a replica failure would provide only little additional benefit.

Redirected put
--------------

Consider the following scenario: In a ring of three nodes, the node
B is the owner for the domain Q. Many entries have been put to Q and
its replicas. After some time, a new node C is added to the ring which
becomes the new owner for the domain Q.

The next put request to the domain Q arrives to the node C. However, since
the node has just started, the domain Q doesn't exists there yet. To the
owner this looks like a put request to a non-existing domain, which should
fail. However, the request is totally valid although the domain owner is
out-of-sync.

To handle cases like this, the owner *redirects* the get request to its
predecessor, if the domain hasn't been created yet on the owner's node,
in hope that the predecessor was the previous owner for the domain and
it could handle the request.

When a predecessor receives a ``ringo_domain:<redir_put>`` request,
it checks whether domain files actually exists on this node. If
they do, it handles the put request similarly to the owner, except
without replication. Otherwise it redirects the request again to its
predecessor. If the redirected put request circulates through the whole
ring, and the owner gets back its own request, the domain is likely to
be non-existent and the request fails.

Eventually the resyncing process will create the domain to the owner, and
copy the previously written entries to it. After this, the put requests are
handled as usual.

Chunking
--------

Ringo doesn't set any limits on the domain size. However, handling an
arbitrarily large domain on a single node is impractical for a number of
reasons:

- Load is distributed unevenly in the ring: If some node happens to contain a
  gigabyte-scale domain whereas others are measured in megabytes, the heavily
  loaded node can easily become a bottleneck for the whole system.

- Likelihood of a catastrophical loss is increased if all data in a domain is
  stored on one disk, regardless of replication.

- Domain size is limited by the largest disk size.

- Indexing and accessing the entries becomes impractical with huge domains.

To alleviate all these issues, domains are split into constant-sized
*chunks*. Currently, the default chunk size is 100MB. When the domain
size exceeds this limit, a new chunk is created, which will handle
subsequent put-requests.

The core Ringo, ``ringo_domain``, doesn't care about chunks. It only
records the size of the domain and if the size exceeds the limit,
it reports this back to the Ringo Gateway. After receiving this reply,
``ringogw`` automatically creates a new domain, after increasing the chunk
number by one. Thus, the ``i`` th chunk for the domain ``domain_name``
is an ordinary domain with the following ``domainid``:

``domainid = md5(i + " " + domain_name)``

As a result, all the data is distributed in 100MB chunks across the
cluster.

Due to chunking, computing the ``domainid`` in the first place for a
put-request becomes more complicated. Since the Ringo Gateway doesn't
know the current chunk number ``i`` in the first place, it can't
compute the correct ``domainid`` for the current chunk. In this case,
``ringogw`` starts trying the request from ``i = 0`` until it succeeds. The
successful chunk ID is stored in an ID cache, so that the next request
doesn't have to repeat the trial.

Getting entries from a domain
=============================

Ringo supports retrieval of stored values based on keys. However,
the key-based retrieval isn't suited for bulk data access, due to
millisecond-scale latencies for accessing individual items. If efficient
access is needed for large amounts of data, the application can directly
access the domain's DB file on disk. Ringo is designed so that reading
the DB file is always safe. As long as the reader omits entries that
don't match their checksum, no partial or corrupted entries are returned.

Values for the given key are fetched from an existing domain with
a GET-request:

``http://ringo/mon/data/domain_name/key``

This call returns a stream of data which contains a list of length-prefixed
values that were stored with the given key. See ``ringogw/py/ringo.py`` for
an easy-to-use Python interface for the stream.

Sometimes it is useful to retrieve only one value for the given key, without
any special encoding. This is possible with the ``?single`` parameter. For
instance, this request

``http://ringo/mon/data/domain_name/key?single``

returns a single value for the given key without any special encoding, so
you can see the results e.g. in a Web browser. No guarantee is given about
which of the possible values is returned.

Index
-----

Ringo maintains an optional per-domain inverted index in real-time in
``ringo_indexdomain``. Extra attention has been paid to make the indices
to consume only a minimal amount of memory, so that the application could
freely store entries as it likes, without having to worry about limited
resources. For details about the index structure, see ``ringo_index``
and ``bin_utils`` that contains some related utility functions.

Since Ringo's indices are based on keys, the number of keys in a domain makes
a big difference for memory consumption and retrieval performance. Since
applications can often roughly estimate the number of keys they will need,
Ringo provides a few different indexing approaches depending on the size of
the key-space. Application can choose the desired approach when creating the
domain.

- If the number of keys is small, say, less than 20, a good approach might be
  to make separate domain for each key. For instance, values for the key A
  could go to domain named ``domain_name/A``. This way you can disable
  indexing for this domain. When the values are needed for A, the application
  can get all values for the domain ``domain_name/A``.

- If the number of keys is large, especially in the cases where each key is
  unique, it makes sense to keep all inverted indices in memory. This mode is
  called ``iblock_cache`` and it is enabled by default.

- If the number of keys is large, but the keys are not accessed uniformly,
  Ringo provides a mode called ``keycache``. In this mode, only inverted
  lists for the most recently used keys are kept in memory. This mode allows
  you to gap the memory consumption for indices.

Ringo doesn't cache the data itself, only indicies that point at the
data. In all these cases, key-value pairs are retrieved from disk, which
may cause many expensive random seeks if the operating system hasn't
cached the DB file in full. Applications are encouraged to use internal
caches that are aware of application-specific usage patterns.

Index blocks
------------

For efficiency, not all the domain's data is stored in one index, which size
might vary greatly depending on the number of keys and values and their
sizes. Instead, the index is split into *index blocks* or *iblocks*, each of
which contains an inverted index for 10000 key-value pairs.

Index blocks are saved to the domain as any other values, except that a
special flag denotes their difference from the user-put values. However,
iblocks are copied to replicas as normal values, which makes it faster for a
replica to take over the owner if it fails.

Note that indices are built lazily. Only when the first get or put-request
is made, the index is opened. This ensures that in the normal case only
the owner contains the index for the domain. However, a replica will build
the index automatically, based on the previously saved iblocks, if requests
are forwarded to it.

Redirected get
--------------

Get-requests are redirected to a previous node similarly to redirected put
requests, if the domain files don't exist on the owner node. Redirection
is handled by ``ringo_domain:<get>``.

Chunks
------

Since any domain chunk may contain the requested key, a get-request
is distributed to all the domain's chunks in parallel. Distribution is
handled by the Ringo Gateway. If a chunk exceeds the chunk size limit,
it replies this fact to ``ringogw`` before it forwards the get request
to its ``ringo_indexdomain`` instance. When ``ringogw`` gets this reply,
it forwards the request also to the next chunk.

This way the request quickly propagates to all available chunks for the
domain, which will process the request in parallel. Note that due to
this parallelism, there is absolutely no guarantee on the order of the
returned values.

Resyncing
=========

Resyncing ensures that replicas and the owner contain the same set
of entries. The ring, which Ringo relies on, may change easily due
to expected and unexpected reasons. In order to be able to address
requests to the nodes that are most likely to respond to them, data must
follow changes in the ring, yet while using only a minimal amount of
precious bandwidth.

In Ringo, the following operations are directly based on resyncing:

- Opportunistic replication: Owner needs no acknowledgements for successful
  replication, as it trusts resyncing to fix any possible problems.
- Node removal / addition in the ring: Nodes may be added and removed to /
  from the ring freely, as resyncing will automatically copy data to
  appropriate nodes.
- Data validity: If an entry in the DB file spontaneously corrupts, resyncing
  will add a valid copy to the file from a replica.

The resyncing algorithm is based on a Merkle- or hash-tree. The tree is built
as follows:

1. A periodical process goes through the DB file
   (``ringo_sync:make_leaf_hashes_and_ids()``).
2. For each entry a leaf L is chosen between 0 and 8191, based on the entry's
   EntryID.
3. A hash value H(L) on the leaf L is updated based on the EntryID.
4. Once all the entries have been processes and leaf hashes computed, a tree
   is built on top of the leaves.
5. For each pair of consequent leaf hashes, a combined hash value is computed
   (``ringo_sync:build_merkle_tree()``).
6. The previous step is repeated for each level of the binary tree, until the
   root is reached. The result is the current Merkle tree for the DB file.

The tree looks somewhat like this:

::

                 Root Hash
                    |
        +-----------+---------+
       ...                   ...         
        |                     |
    H(H0, H1)          H(H8190, H8191)  
        |                     |
     +------+            +--------+
     |      |            |        |
     H0     H1   ....  H8190    H8191


Merkle-tree provides an efficient way to check if two ordered sets
of items are equal, and if they aren't, which of the items differ. If
two DB files contain exactly the same set of entries, the roots of the
corresponding Merkle trees are necessarily equal. If some of the entries
are missing from either tree, we can traverse the tree downwards to find
the leaves that differ.

In Ringo, this method is used to resync replicas with the owner. To avoid
K^2 negotiations between K replicas, each replica resyncs its contents
only with the owner. Eventually, all missing entries will flow through
the owner to all needing replicas.

Missing entries are exchanged through the domain's *inbox* and *outbox*.
Inbox contains items that are missing from this node, and wait to be written
to the DB file. Outbox contains entry IDs that are requested from this node,
and wait to be fetched from the DB file. The boxes serve as buffers to avoid
lots of individual, expensive, random accesses to the DB file.

On the replica-end the synchronization process, started periodically by
``ringo_syncdomain:resync()``, proceeds as follows:

1. Current inbox is retrieved with a ``ringo_domain:<flush_syncbox>>``
   request.
2. Leaf hashes are re-computed by ``ringo_sync:make_leaf_hashes_and_ids()``.
3. Inbox is compared against the current entry IDs. Only those entries which
   don't exist in the DB already are appended to it in
   ``ringo_sync:flush_sync_inbox()``.
4. A Merkle-tree is built for the new DB, including the just added entries,
   by ``ringo_sync:build_merkle_tree()``.
5. Current owner for this domain is queried in
   ``ringo_syncdomain:find_owner()`` using the ``ringo_domain:<find_owner>``
   request.
6. Replica's Merkle-tree is compared with that of the owner's, using
   the ``ringo_domain:<sync_tree>`` request. The result is a list of
   entry IDs that are missing either from the owner or the replica.
7. Replica sends a ``ringo_domain:<sync_pack>`` request to the owner that
   contains the list of missing entry IDs. The owner inserts entryIDs that
   are missing from the replica to its outbox. It requests entries that are
   missing from it from the replica with a ``ringo_domain:<sync_get>``
   request. This request adds the entryIDs to the replica's outbox.
8. Outbox is processed by ``ringo_syncdomain:flush_sync_outbox()``. It takes
   the current outbox, goes through the DB file, and sends the requested entries
   to the requester when a matching entry is found with a 
   ``ringo_domain:<sync_put>`` call.

The owner performs only steps 1-4 and 8.

If the resyncing process fails at any point, some entries may fail to
get resynced correctly. However, once the resyncing process starts again
after a predefined interval, the difference is detected again and the
same process repeats. The remarkable fact is that there aren't any hot spots
in the resyncing process, whose failure might compromise the DB.

Large entries
-------------

The above resyncing process guarantees that all the DB files belonging
to a domain contain eventually the same set of entries. However, large,
external values which are stored in separate files are handled by another
process.

If ``ringo_syncdomain:flush_sync_inbox()`` notices that an entry contains an
external value, it sends a message to ``ringo_external:fetch_external()`` that
contains a queue of external files that are waiting to be copied. This
process makes sure that the external file will be eventually copied to the
node.

A separate, periodically running process in
``ringo_external:check_external()`` goes through the DB file. For
each external entry it checks if the corresponding external file
actually exists.  If it doesn't, it is requested to be copied by
``ringo_external:fetch_external()``.


References
==========

.. [Dynamo] *Dynamo: Amazon's Highly Available Key-value Store*, http://s3.amazonaws.com/AllThingsDistributed/sosp/amazon-dynamo-sosp2007.pdf 


Generated from ``doc/ringodoc.txt`` on |date|.
